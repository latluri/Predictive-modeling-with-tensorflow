{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the imdb rating from the movie data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This script is written by Atluri Laxmi Narayana\n",
    "\n",
    "This script reads the file \"movie_metadata.csv\" to predict the imdb score \n",
    "\n",
    "This code generates linear model in tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\final\\\\movie_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target is imdb_score\n",
    "\n",
    "Removing the data where imdb_score is na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[np.isfinite(df['imdb_score'])]\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in getting a count of genres and the plotkey words for the movie and how this count impacts the imdb rating\n",
    "Further, we are interested in learning the effect of title length and the first alphabet/numerical of the title and how it impacts our target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"genres_count\"]=df.genres.str.split('|', 0).apply(lambda x: len(x))\n",
    "df[\"plotkeywords_count\"]=df.plot_keywords.str.split('|', 0).apply(lambda x: len(str(x)))\n",
    "df[\"genres_array\"]=df.genres.str.split('|', 0)\n",
    "df[\"plotkeyword_1\"],df[\"plotkeyword_others\"]=df.plot_keywords.str.split('|', 1).str\n",
    "df[\"movie_title_word_length\"]=df.movie_title.str.split(' ', 0).apply(lambda x: len(x))\n",
    "df[\"movie_first_alpha_numeric\"]=pd.DataFrame(df.movie_title.str.split(' ',1).tolist(),columns = ['first','other'])[\"first\"].str[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colums ignored are\n",
    "movie_imdb_link\n",
    "plot_keywords\n",
    "movie_title\n",
    "genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(\"genres\",1)\n",
    "df=df.drop(\"movie_imdb_link\",1)\n",
    "df=df.drop(\"movie_title\",1)\n",
    "df=df.drop(\"plot_keywords\",1)\n",
    "df=df.drop(\"plotkeyword_others\",1)\n",
    "df.aspect_ratio=df.aspect_ratio.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the target rating into bins\n",
    "This is done because the ratings in the data vary by .1 but this does not necessarily have a huge impact with the audiance\n",
    "\n",
    "For example, a movie with 9.1 rating is as good as a movie with 9.2 rating.So, Ideally we will have to convert imdb rating into fewer bins\n",
    "\n",
    "Since linear model accepts target of type boolean (n_classes= 2). we have converted imdb score into a column which indicates if its greater than 5\n",
    "\n",
    "Any movie with rating greater than 5 is indicated with 1\n",
    "Less than 5 is indicated with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.imdb_score=pd.cut(df['imdb_score'],[0,5,10],labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test subsets. 60 train 40 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df.sample(frac=.6, random_state=1232)\n",
    "x_test = df.drop(x_train.index)\n",
    "\n",
    "x_train.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n",
    "x_test.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_CSV_COLUMNS =df.columns.values.tolist()\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in df.dtypes:\n",
    "   if(column==\"int64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0])\n",
    "   elif(column==\"float64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0.00])\n",
    "   else:\n",
    "        _CSV_COLUMN_DEFAULTS.append([''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quality Check. The value of below code must be true \n",
    "len(_CSV_COLUMNS)==len(_CSV_COLUMN_DEFAULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaing the features of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres_count = tf.feature_column.numeric_column ('genres_count')\n",
    "plotkeywords_count = tf.feature_column.numeric_column ('plotkeywords_count')\n",
    "color=tf.feature_column.categorical_column_with_vocabulary_list('color', ['Color' ' Black and White' ''])\n",
    "language=tf.feature_column.categorical_column_with_vocabulary_list('language',['English' 'Cantonese' 'Romanian' 'Spanish' 'Japanese' 'Mandarin' 'Norwegian' 'Portuguese' 'Indonesian' 'French' 'Dutch' 'Polish' 'Danish' 'Hindi' '' 'German' 'Hebrew' 'Dari' 'Hungarian' 'None' 'Arabic' 'Swedish' 'Korean' 'Zulu' 'Chinese' 'Persian' 'Italian' 'Aboriginal' 'Vietnamese' 'Telugu' 'Thai' 'Filipino' 'Mongolian' 'Aramaic' 'Kazakh' 'Maya' 'Czech' 'Dzongkha' 'Russian' 'Icelandic' 'Bosnian' 'Greek'] )\n",
    "country=tf.feature_column.categorical_column_with_vocabulary_list('country', ['USA' 'Germany' 'Hong Kong' 'UK' 'Romania' 'France' 'Mexico' 'Japan' 'China' 'Philippines' 'Norway' 'Brazil' 'Australia' 'New Zealand' 'Indonesia' 'Netherlands' 'Poland' 'Canada' 'Spain' 'Denmark' 'India' 'Argentina' 'Russia' 'Ireland' 'Czech Republic' 'Israel' 'Hungary' 'South Korea' 'South Africa' 'Sweden' 'Official site' 'Italy' 'Taiwan' 'Cameroon' 'Thailand' 'Georgia' 'Iran' 'New Line' 'Aruba' 'Afghanistan' 'Belgium' 'Iceland' 'Greece' 'Chile' 'Peru' 'West Germany' 'Colombia' 'Finland'])\n",
    "content_rating=tf.feature_column.categorical_column_with_vocabulary_list('content_rating',['PG-13' 'R' 'PG' 'G' 'Not Rated' 'Approved' 'Unrated' '' 'TV-MA' 'Passed' 'X' 'M' 'GP' 'NC-17'] )\n",
    "aspect_ratio = tf.feature_column.categorical_column_with_vocabulary_list('aspect_ratio',['2.35','1.85','2.39','1.66','','1.77','1.33','2.','2.76','1.78','1.37','2.2','1.75','2.55','2.4','2.24','1.5','16.','1.44','1.18'])\n",
    "\n",
    "director_name = tf.feature_column.categorical_column_with_hash_bucket('director_name', hash_bucket_size=5000)\n",
    "actor_2_name = tf.feature_column.categorical_column_with_hash_bucket('actor_2_name', hash_bucket_size=5000)\n",
    "actor_1_name = tf.feature_column.categorical_column_with_hash_bucket('actor_1_name', hash_bucket_size=5000)\n",
    "actor_3_name = tf.feature_column.categorical_column_with_hash_bucket('actor_3_name', hash_bucket_size=5000)\n",
    "genres_array = tf.feature_column.categorical_column_with_hash_bucket('genres_array', hash_bucket_size=len(set(functools.reduce(list.__add__, df.genres_array))))\n",
    "plotkeyword_1 = tf.feature_column.categorical_column_with_hash_bucket('plotkeyword_1', hash_bucket_size=len(set(df.plotkeyword_1)))\n",
    "movie_first_alpha_numeric = tf.feature_column.categorical_column_with_hash_bucket('movie_first_alpha_numeric', hash_bucket_size=len(set(df.movie_first_alpha_numeric)))\n",
    "num_critic_for_reviews = tf.feature_column.numeric_column ('num_critic_for_reviews')\n",
    "duration = tf.feature_column.numeric_column ('duration')\n",
    "director_facebook_likes = tf.feature_column.numeric_column ('director_facebook_likes')\n",
    "actor_3_facebook_likes = tf.feature_column.numeric_column ('actor_3_facebook_likes')\n",
    "actor_1_facebook_likes = tf.feature_column.numeric_column ('actor_1_facebook_likes')\n",
    "gross = tf.feature_column.numeric_column ('gross')\n",
    "num_voted_users = tf.feature_column.numeric_column ('num_voted_users')\n",
    "cast_total_facebook_likes = tf.feature_column.numeric_column ('cast_total_facebook_likes')\n",
    "facenumber_in_poster = tf.feature_column.numeric_column ('facenumber_in_poster')\n",
    "num_user_for_reviews = tf.feature_column.numeric_column ('num_user_for_reviews')\n",
    "budget = tf.feature_column.numeric_column ('budget')\n",
    "title_year = tf.feature_column.numeric_column ('title_year')\n",
    "actor_2_facebook_likes = tf.feature_column.numeric_column ('actor_2_facebook_likes')\n",
    "movie_facebook_likes = tf.feature_column.numeric_column ('movie_facebook_likes')\n",
    "movie_title_word_length = tf.feature_column.numeric_column ('movie_title_word_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating tensor flow buckets based on the numeric columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_critic_for_reviews_buckets = tf.feature_column.bucketized_column(num_critic_for_reviews,boundaries=[35.0, 60.0, 81.0, 105.0, 130.0, 159.0, 193.0, 241.0, 323.0, 813.0])\n",
    "duration_buckets = tf.feature_column.bucketized_column(duration,boundaries=[88.0, 93.0, 97.0, 101.0, 105.0, 110.0, 116.0, 123.0, 135.0, 334.0])\n",
    "director_facebook_likes_buckets = tf.feature_column.bucketized_column(director_facebook_likes,boundaries=[4.0, 15.0, 32.0, 57.0, 98.0, 165.0, 301.0, 571.0, 23000.0])\n",
    "actor_3_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_3_facebook_likes,boundaries=[48.0, 122.0, 218.0, 311.0, 416.0, 511.0, 612.0, 745.0, 924.5, 23000.0])\n",
    "actor_1_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_1_facebook_likes,boundaries=[353.0, 600.8, 794.2, 939.0, 1000.0, 3000.0, 11000.0, 14000.0, 20000.0, 640000.0])\n",
    "gross_buckets = tf.feature_column.bucketized_column(gross,boundaries=[382601.8, 3023265.0, 8131359.8, 15802119.0, 25517500.0, 36565306.4, 51798114.8, 75601728.0, 125025163.2, 760505847.0])\n",
    "num_voted_users_buckets = tf.feature_column.bucketized_column(num_voted_users,boundaries=[4553.4, 11232.6, 20003.2, 31266.6, 46396.0, 66462.8, 93759.4, 145332.6, 247440.2, 1689764.0])\n",
    "cast_total_facebook_likes_buckets = tf.feature_column.bucketized_column(cast_total_facebook_likes,boundaries=[725.8, 1384.2, 2049.6, 2731.0, 3697.0, 5708.8, 13332.2, 18452.8, 28099.6, 656730.0])\n",
    "facenumber_in_poster_buckets = tf.feature_column.bucketized_column(facenumber_in_poster,boundaries=[1.0, 2.0, 4.0, 43.0])\n",
    "num_user_for_reviews_buckets = tf.feature_column.bucketized_column(num_user_for_reviews,boundaries=[40.0, 77.0, 111.0, 147.0, 190.0, 247.0, 320.9, 442.6, 687.0, 5060.0])\n",
    "budget_buckets = tf.feature_column.bucketized_column(budget,boundaries=[2600000.0, 7000000.0, 12000000.0, 17000000.0, 24000000.0, 30000000.0, 40000000.0, 60000000.0, 93000000.0, 12215500000.0])\n",
    "title_year_buckets = tf.feature_column.bucketized_column(title_year,boundaries=[1993.0, 1998.0, 2000.0, 2003.0, 2005.0, 2007.0, 2009.0, 2011.0, 2013.0, 2016.0])\n",
    "actor_2_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_2_facebook_likes,boundaries=[114.2, 269.0, 417.6, 542.8, 650.0, 793.2, 899.0, 1000.0, 5000.0, 137000.0])\n",
    "movie_facebook_likes_buckets = tf.feature_column.bucketized_column(movie_facebook_likes,boundaries=[182.0, 613.8, 1000.0, 14000.0, 27200.0, 349000.0])\n",
    "genres_count_buckets = tf.feature_column.bucketized_column(genres_count,boundaries=[2.0, 3.0, 4.0, 8.0])\n",
    "plotkeywords_count_buckets = tf.feature_column.bucketized_column(plotkeywords_count,boundaries=[51.0, 55.0, 59.0, 62.0, 66.0, 70.0, 74.6, 80.0, 90.0, 165.0])\n",
    "movie_title_word_length_buckets = tf.feature_column.bucketized_column(movie_title_word_length,boundaries=[2.0, 3.0, 4.0, 5.0, 13.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "  def parse_csv(data_file):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(data_file, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('imdb_score')\n",
    "    return features,tf.equal(labels, 1)\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  \n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the base columns\n",
    "base_columns = [genres_count_buckets,plotkeywords_count_buckets,plotkeyword_1,color,language,country,content_rating,director_name,actor_2_name,actor_1_name,actor_3_name,num_critic_for_reviews_buckets,duration_buckets,director_facebook_likes_buckets,actor_3_facebook_likes_buckets,actor_2_facebook_likes_buckets,actor_1_facebook_likes_buckets,num_voted_users_buckets,facenumber_in_poster_buckets,budget_buckets,title_year_buckets,gross_buckets,aspect_ratio]\n",
    "#note user reviews and movie facebook likes and cast facebook likes are not considered for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'tmp\\\\modelL2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002951B6F0D68>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#defining the linear model\n",
    "model_dir = \"tmp\\\\modelL2\"\n",
    "\n",
    "model = tf.estimator.LinearClassifier(model_dir=model_dir, feature_columns=base_columns,optimizer=tf.train.FtrlOptimizer(learning_rate=0.1,l1_regularization_strength=1.0,l2_regularization_strength=1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from tmp\\modelL2\\model.ckpt-12104\n",
      "INFO:tensorflow:Saving checkpoints for 12105 into tmp\\modelL2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.488129, step = 12105\n",
      "INFO:tensorflow:global_step/sec: 50.0397\n",
      "INFO:tensorflow:loss = 0.625223, step = 12205 (1.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.325\n",
      "INFO:tensorflow:loss = 1.70006, step = 12305 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.665\n",
      "INFO:tensorflow:loss = 1.58838, step = 12405 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.581\n",
      "INFO:tensorflow:loss = 0.771451, step = 12505 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.394\n",
      "INFO:tensorflow:loss = 0.706963, step = 12605 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.339\n",
      "INFO:tensorflow:loss = 0.176705, step = 12705 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.305\n",
      "INFO:tensorflow:loss = 0.272893, step = 12805 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.379\n",
      "INFO:tensorflow:loss = 0.441515, step = 12905 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.582\n",
      "INFO:tensorflow:loss = 0.35193, step = 13005 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.34\n",
      "INFO:tensorflow:loss = 3.41464, step = 13105 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.401\n",
      "INFO:tensorflow:loss = 0.152488, step = 13205 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.097\n",
      "INFO:tensorflow:loss = 0.824137, step = 13305 (0.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.055\n",
      "INFO:tensorflow:loss = 0.336548, step = 13405 (0.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.798\n",
      "INFO:tensorflow:loss = 0.171355, step = 13505 (0.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.098\n",
      "INFO:tensorflow:loss = 0.785453, step = 13605 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.976\n",
      "INFO:tensorflow:loss = 0.903364, step = 13705 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.975\n",
      "INFO:tensorflow:loss = 1.18829, step = 13805 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.056\n",
      "INFO:tensorflow:loss = 0.375154, step = 13905 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.714\n",
      "INFO:tensorflow:loss = 0.179718, step = 14005 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.528\n",
      "INFO:tensorflow:loss = 4.56074, step = 14105 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.234\n",
      "INFO:tensorflow:loss = 1.62776, step = 14205 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.9\n",
      "INFO:tensorflow:loss = 0.516688, step = 14305 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.338\n",
      "INFO:tensorflow:loss = 0.378442, step = 14405 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.047\n",
      "INFO:tensorflow:loss = 0.49307, step = 14505 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.19\n",
      "INFO:tensorflow:loss = 0.800085, step = 14605 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.047\n",
      "INFO:tensorflow:loss = 0.434978, step = 14705 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.262\n",
      "INFO:tensorflow:loss = 3.4963, step = 14805 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.768\n",
      "INFO:tensorflow:loss = 0.713178, step = 14905 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.662\n",
      "INFO:tensorflow:loss = 0.825415, step = 15005 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.234\n",
      "INFO:tensorflow:loss = 0.560724, step = 15105 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.261\n",
      "INFO:tensorflow:loss = 0.384643, step = 15205 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.262\n",
      "INFO:tensorflow:loss = 1.93269, step = 15305 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.811\n",
      "INFO:tensorflow:loss = 0.200615, step = 15405 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.81\n",
      "INFO:tensorflow:loss = 0.804893, step = 15505 (0.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.575\n",
      "INFO:tensorflow:loss = 0.203623, step = 15605 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.638\n",
      "INFO:tensorflow:loss = 0.60914, step = 15705 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 271.546\n",
      "INFO:tensorflow:loss = 0.678859, step = 15805 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.236\n",
      "INFO:tensorflow:loss = 0.461257, step = 15905 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.231\n",
      "INFO:tensorflow:loss = 0.270192, step = 16005 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.242\n",
      "INFO:tensorflow:loss = 0.32984, step = 16105 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.285\n",
      "INFO:tensorflow:loss = 0.478274, step = 16205 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.242\n",
      "INFO:tensorflow:loss = 0.205308, step = 16305 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.242\n",
      "INFO:tensorflow:loss = 0.219657, step = 16405 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.306\n",
      "INFO:tensorflow:loss = 3.3889, step = 16505 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.235\n",
      "INFO:tensorflow:loss = 0.774021, step = 16605 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.232\n",
      "INFO:tensorflow:loss = 3.93242, step = 16705 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.077\n",
      "INFO:tensorflow:loss = 0.88449, step = 16805 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.306\n",
      "INFO:tensorflow:loss = 0.0418013, step = 16905 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.234\n",
      "INFO:tensorflow:loss = 1.33971, step = 17005 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.53\n",
      "INFO:tensorflow:loss = 0.428612, step = 17105 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.648\n",
      "INFO:tensorflow:loss = 0.229441, step = 17205 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.378\n",
      "INFO:tensorflow:loss = 0.523206, step = 17305 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.262\n",
      "INFO:tensorflow:loss = 0.704225, step = 17405 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.501\n",
      "INFO:tensorflow:loss = 0.240667, step = 17505 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.236\n",
      "INFO:tensorflow:loss = 0.408278, step = 17605 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.286\n",
      "INFO:tensorflow:loss = 1.52514, step = 17705 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.775\n",
      "INFO:tensorflow:loss = 0.401938, step = 17805 (0.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.461\n",
      "INFO:tensorflow:loss = 0.456455, step = 17905 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 318.246\n",
      "INFO:tensorflow:loss = 0.307949, step = 18005 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.262\n",
      "INFO:tensorflow:loss = 0.680839, step = 18105 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.286\n",
      "INFO:tensorflow:loss = 0.589568, step = 18205 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 327.636\n",
      "INFO:tensorflow:loss = 1.20464, step = 18305 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.461\n",
      "INFO:tensorflow:loss = 0.681873, step = 18405 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 341.056\n",
      "INFO:tensorflow:loss = 0.398585, step = 18505 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.03\n",
      "INFO:tensorflow:loss = 0.371577, step = 18605 (0.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.99\n",
      "INFO:tensorflow:loss = 0.856885, step = 18705 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.891\n",
      "INFO:tensorflow:loss = 0.275733, step = 18805 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.661\n",
      "INFO:tensorflow:loss = 0.254836, step = 18905 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.231\n",
      "INFO:tensorflow:loss = 0.333579, step = 19005 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.422\n",
      "INFO:tensorflow:loss = 0.308565, step = 19105 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 337.598\n",
      "INFO:tensorflow:loss = 0.274748, step = 19205 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.629\n",
      "INFO:tensorflow:loss = 0.729064, step = 19305 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.423\n",
      "INFO:tensorflow:loss = 0.335777, step = 19405 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.9\n",
      "INFO:tensorflow:loss = 0.523096, step = 19505 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 324.445\n",
      "INFO:tensorflow:loss = 0.378212, step = 19605 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.262\n",
      "INFO:tensorflow:loss = 0.139395, step = 19705 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.235\n",
      "INFO:tensorflow:loss = 0.335071, step = 19805 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.799\n",
      "INFO:tensorflow:loss = 0.230169, step = 19905 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.235\n",
      "INFO:tensorflow:loss = 0.294398, step = 20005 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.295\n",
      "INFO:tensorflow:loss = 0.533183, step = 20105 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.280614, step = 20205 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.649\n",
      "INFO:tensorflow:loss = 0.466659, step = 20305 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.189\n",
      "INFO:tensorflow:loss = 0.811704, step = 20405 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.492\n",
      "INFO:tensorflow:loss = 0.857785, step = 20505 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.304\n",
      "INFO:tensorflow:loss = 0.508547, step = 20605 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.813\n",
      "INFO:tensorflow:loss = 0.143761, step = 20705 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 0.721685, step = 20805 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.9\n",
      "INFO:tensorflow:loss = 0.459039, step = 20905 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.525\n",
      "INFO:tensorflow:loss = 0.36966, step = 21005 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.262\n",
      "INFO:tensorflow:loss = 0.396955, step = 21105 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.304\n",
      "INFO:tensorflow:loss = 0.294035, step = 21205 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 0.356061, step = 21305 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 318.246\n",
      "INFO:tensorflow:loss = 0.235201, step = 21405 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.9\n",
      "INFO:tensorflow:loss = 0.352595, step = 21505 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.9\n",
      "INFO:tensorflow:loss = 0.272046, step = 21605 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.699\n",
      "INFO:tensorflow:loss = 0.574919, step = 21705 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.03\n",
      "INFO:tensorflow:loss = 0.633752, step = 21805 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.736\n",
      "INFO:tensorflow:loss = 0.219383, step = 21905 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 317.234\n",
      "INFO:tensorflow:loss = 0.284645, step = 22005 (0.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.279\n",
      "INFO:tensorflow:loss = 0.37919, step = 22105 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.4\n",
      "INFO:tensorflow:loss = 0.4631, step = 22205 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.565\n",
      "INFO:tensorflow:loss = 0.319789, step = 22305 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.736\n",
      "INFO:tensorflow:loss = 0.189328, step = 22405 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.736\n",
      "INFO:tensorflow:loss = 0.221881, step = 22505 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.422\n",
      "INFO:tensorflow:loss = 0.175623, step = 22605 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.283\n",
      "INFO:tensorflow:loss = 0.38979, step = 22705 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.318\n",
      "INFO:tensorflow:loss = 0.175346, step = 22805 (0.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.21\n",
      "INFO:tensorflow:loss = 0.172898, step = 22905 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.573\n",
      "INFO:tensorflow:loss = 0.199728, step = 23005 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.395\n",
      "INFO:tensorflow:loss = 0.191008, step = 23105 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 334.211\n",
      "INFO:tensorflow:loss = 0.451988, step = 23205 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.714\n",
      "INFO:tensorflow:loss = 0.0870338, step = 23305 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.798\n",
      "INFO:tensorflow:loss = 0.451633, step = 23405 (0.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.242\n",
      "INFO:tensorflow:loss = 1.03436, step = 23505 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.891\n",
      "INFO:tensorflow:loss = 0.240301, step = 23605 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.457\n",
      "INFO:tensorflow:loss = 0.392776, step = 23705 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.222\n",
      "INFO:tensorflow:loss = 0.199606, step = 23805 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 339.896\n",
      "INFO:tensorflow:loss = 0.181023, step = 23905 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.398\n",
      "INFO:tensorflow:loss = 0.448984, step = 24005 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.185\n",
      "INFO:tensorflow:loss = 0.141526, step = 24105 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.378\n",
      "INFO:tensorflow:loss = 1.97561, step = 24205 (0.275 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24208 into tmp\\modelL2\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0625031.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x2951b6f0c50>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model \n",
    "model.train(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", 40, True,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-06-16:37:53\n",
      "INFO:tensorflow:Restoring parameters from tmp\\modelL2\\model.ckpt-24208\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-06-16:38:04\n",
      "INFO:tensorflow:Saving dict for global step 24208: accuracy = 0.950917, accuracy_baseline = 0.895885, auc = 0.909848, auc_precision_recall = 0.984473, average_loss = 0.16652, global_step = 24208, label/mean = 0.895885, loss = 1.66273, prediction/mean = 0.900645\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "results = model.evaluate(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", 1, False, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.950917\n",
      "accuracy_baseline: 0.895885\n",
      "auc: 0.909848\n",
      "auc_precision_recall: 0.984473\n",
      "average_loss: 0.16652\n",
      "global_step: 24208\n",
      "label/mean: 0.895885\n",
      "loss: 1.66273\n",
      "prediction/mean: 0.900645\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(results):\n",
    "  print('%s: %s' % (key, results[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To check predicted results \n",
    "#predict_results = model.predict(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", 1, True,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
