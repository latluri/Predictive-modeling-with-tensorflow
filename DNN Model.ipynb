{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the imdb rating from the movie data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This script is written by Atluri Laxmi Narayana\n",
    "\n",
    "This script reads the file \"movie_metadata.csv\" to predict the imdb score \n",
    "\n",
    "This code generates DNN model in tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\final\\\\movie_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target is imdb score\n",
    "\n",
    "Removing the data where imdb score is na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[np.isfinite(df['imdb_score'])]\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in getting a count of genres and the plotkey words for the movie and how this count impacts the imdb rating\n",
    "Further, we are interested in learning the effect of title length and the first alphabet/numerical of the title and how it impacts our target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"genres_count\"]=df.genres.str.split('|', 0).apply(lambda x: len(x))\n",
    "df[\"plotkeywords_count\"]=df.plot_keywords.str.split('|', 0).apply(lambda x: len(str(x)))\n",
    "df[\"genres_array\"]=df.genres.str.split('|', 0)\n",
    "df[\"plotkeyword_1\"],df[\"plotkeyword_others\"]=df.plot_keywords.str.split('|', 1).str\n",
    "df[\"movie_title_word_length\"]=df.movie_title.str.split(' ', 0).apply(lambda x: len(x))\n",
    "df[\"movie_first_alpha_numeric\"]=pd.DataFrame(df.movie_title.str.split(' ',1).tolist(),columns = ['first','other'])[\"first\"].str[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colums ignored are\n",
    "movie_imdb_link\n",
    "plot_keywords\n",
    "movie_title\n",
    "genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(\"genres\",1)\n",
    "df=df.drop(\"movie_imdb_link\",1)\n",
    "df=df.drop(\"movie_title\",1)\n",
    "df=df.drop(\"plot_keywords\",1)\n",
    "df=df.drop(\"plotkeyword_others\",1)\n",
    "df.aspect_ratio=df.aspect_ratio.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the target rating into bins\n",
    "This is done because the ratings in the data vary by .1 but this does not necessarily have a huge impact with the audiance\n",
    "For example, a movie with 9.1 rating is as good as a movie with 9.2 rating.So, Ideally we will have to convert imdb rating into fewer bins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.imdb_score=pd.cut(df['imdb_score'],[0,2.5,5,7.5,10],labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test subsets. 60 train 40 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df.sample(frac=.6, random_state=1232)\n",
    "x_test = df.drop(x_train.index)\n",
    "\n",
    "x_train.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n",
    "x_test.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_CSV_COLUMNS =df.columns.values.tolist()\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in df.dtypes:\n",
    "   if(column==\"int64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0])\n",
    "   elif(column==\"float64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0.00])\n",
    "   else:\n",
    "        _CSV_COLUMN_DEFAULTS.append([''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quality Check. The value of below code must be true \n",
    "len(_CSV_COLUMNS)==len(_CSV_COLUMN_DEFAULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaing the features of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres_count = tf.feature_column.numeric_column ('genres_count')\n",
    "plotkeywords_count = tf.feature_column.numeric_column ('plotkeywords_count')\n",
    "color=tf.feature_column.categorical_column_with_vocabulary_list('color', ['Color' ' Black and White' ''])\n",
    "language=tf.feature_column.categorical_column_with_vocabulary_list('language',['English' 'Cantonese' 'Romanian' 'Spanish' 'Japanese' 'Mandarin' 'Norwegian' 'Portuguese' 'Indonesian' 'French' 'Dutch' 'Polish' 'Danish' 'Hindi' '' 'German' 'Hebrew' 'Dari' 'Hungarian' 'None' 'Arabic' 'Swedish' 'Korean' 'Zulu' 'Chinese' 'Persian' 'Italian' 'Aboriginal' 'Vietnamese' 'Telugu' 'Thai' 'Filipino' 'Mongolian' 'Aramaic' 'Kazakh' 'Maya' 'Czech' 'Dzongkha' 'Russian' 'Icelandic' 'Bosnian' 'Greek'] )\n",
    "country=tf.feature_column.categorical_column_with_vocabulary_list('country', ['USA' 'Germany' 'Hong Kong' 'UK' 'Romania' 'France' 'Mexico' 'Japan' 'China' 'Philippines' 'Norway' 'Brazil' 'Australia' 'New Zealand' 'Indonesia' 'Netherlands' 'Poland' 'Canada' 'Spain' 'Denmark' 'India' 'Argentina' 'Russia' 'Ireland' 'Czech Republic' 'Israel' 'Hungary' 'South Korea' 'South Africa' 'Sweden' 'Official site' 'Italy' 'Taiwan' 'Cameroon' 'Thailand' 'Georgia' 'Iran' 'New Line' 'Aruba' 'Afghanistan' 'Belgium' 'Iceland' 'Greece' 'Chile' 'Peru' 'West Germany' 'Colombia' 'Finland'])\n",
    "content_rating=tf.feature_column.categorical_column_with_vocabulary_list('content_rating',['PG-13' 'R' 'PG' 'G' 'Not Rated' 'Approved' 'Unrated' '' 'TV-MA' 'Passed' 'X' 'M' 'GP' 'NC-17'] )\n",
    "aspect_ratio = tf.feature_column.categorical_column_with_vocabulary_list('aspect_ratio',['2.35','1.85','2.39','1.66','','1.77','1.33','2.','2.76','1.78','1.37','2.2','1.75','2.55','2.4','2.24','1.5','16.','1.44','1.18'])\n",
    "\n",
    "director_name = tf.feature_column.categorical_column_with_hash_bucket('director_name', hash_bucket_size=5000)\n",
    "actor_2_name = tf.feature_column.categorical_column_with_hash_bucket('actor_2_name', hash_bucket_size=5000)\n",
    "actor_1_name = tf.feature_column.categorical_column_with_hash_bucket('actor_1_name', hash_bucket_size=5000)\n",
    "actor_3_name = tf.feature_column.categorical_column_with_hash_bucket('actor_3_name', hash_bucket_size=5000)\n",
    "genres_array = tf.feature_column.categorical_column_with_hash_bucket('genres_array', hash_bucket_size=len(set(functools.reduce(list.__add__, df.genres_array))))\n",
    "plotkeyword_1 = tf.feature_column.categorical_column_with_hash_bucket('plotkeyword_1', hash_bucket_size=len(set(df.plotkeyword_1)))\n",
    "movie_first_alpha_numeric = tf.feature_column.categorical_column_with_hash_bucket('movie_first_alpha_numeric', hash_bucket_size=len(set(df.movie_first_alpha_numeric)))\n",
    "num_critic_for_reviews = tf.feature_column.numeric_column ('num_critic_for_reviews')\n",
    "duration = tf.feature_column.numeric_column ('duration')\n",
    "director_facebook_likes = tf.feature_column.numeric_column ('director_facebook_likes')\n",
    "actor_3_facebook_likes = tf.feature_column.numeric_column ('actor_3_facebook_likes')\n",
    "actor_1_facebook_likes = tf.feature_column.numeric_column ('actor_1_facebook_likes')\n",
    "gross = tf.feature_column.numeric_column ('gross')\n",
    "num_voted_users = tf.feature_column.numeric_column ('num_voted_users')\n",
    "cast_total_facebook_likes = tf.feature_column.numeric_column ('cast_total_facebook_likes')\n",
    "facenumber_in_poster = tf.feature_column.numeric_column ('facenumber_in_poster')\n",
    "num_user_for_reviews = tf.feature_column.numeric_column ('num_user_for_reviews')\n",
    "budget = tf.feature_column.numeric_column ('budget')\n",
    "title_year = tf.feature_column.numeric_column ('title_year')\n",
    "actor_2_facebook_likes = tf.feature_column.numeric_column ('actor_2_facebook_likes')\n",
    "movie_facebook_likes = tf.feature_column.numeric_column ('movie_facebook_likes')\n",
    "movie_title_word_length = tf.feature_column.numeric_column ('movie_title_word_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tensor flow buckets based on the numeric columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_critic_for_reviews_buckets = tf.feature_column.bucketized_column(num_critic_for_reviews,boundaries=[35.0, 60.0, 81.0, 105.0, 130.0, 159.0, 193.0, 241.0, 323.0, 813.0])\n",
    "duration_buckets = tf.feature_column.bucketized_column(duration,boundaries=[88.0, 93.0, 97.0, 101.0, 105.0, 110.0, 116.0, 123.0, 135.0, 334.0])\n",
    "director_facebook_likes_buckets = tf.feature_column.bucketized_column(director_facebook_likes,boundaries=[4.0, 15.0, 32.0, 57.0, 98.0, 165.0, 301.0, 571.0, 23000.0])\n",
    "actor_3_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_3_facebook_likes,boundaries=[48.0, 122.0, 218.0, 311.0, 416.0, 511.0, 612.0, 745.0, 924.5, 23000.0])\n",
    "actor_1_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_1_facebook_likes,boundaries=[353.0, 600.8, 794.2, 939.0, 1000.0, 3000.0, 11000.0, 14000.0, 20000.0, 640000.0])\n",
    "gross_buckets = tf.feature_column.bucketized_column(gross,boundaries=[382601.8, 3023265.0, 8131359.8, 15802119.0, 25517500.0, 36565306.4, 51798114.8, 75601728.0, 125025163.2, 760505847.0])\n",
    "num_voted_users_buckets = tf.feature_column.bucketized_column(num_voted_users,boundaries=[4553.4, 11232.6, 20003.2, 31266.6, 46396.0, 66462.8, 93759.4, 145332.6, 247440.2, 1689764.0])\n",
    "cast_total_facebook_likes_buckets = tf.feature_column.bucketized_column(cast_total_facebook_likes,boundaries=[725.8, 1384.2, 2049.6, 2731.0, 3697.0, 5708.8, 13332.2, 18452.8, 28099.6, 656730.0])\n",
    "facenumber_in_poster_buckets = tf.feature_column.bucketized_column(facenumber_in_poster,boundaries=[1.0, 2.0, 4.0, 43.0])\n",
    "num_user_for_reviews_buckets = tf.feature_column.bucketized_column(num_user_for_reviews,boundaries=[40.0, 77.0, 111.0, 147.0, 190.0, 247.0, 320.9, 442.6, 687.0, 5060.0])\n",
    "budget_buckets = tf.feature_column.bucketized_column(budget,boundaries=[2600000.0, 7000000.0, 12000000.0, 17000000.0, 24000000.0, 30000000.0, 40000000.0, 60000000.0, 93000000.0, 12215500000.0])\n",
    "title_year_buckets = tf.feature_column.bucketized_column(title_year,boundaries=[1993.0, 1998.0, 2000.0, 2003.0, 2005.0, 2007.0, 2009.0, 2011.0, 2013.0, 2016.0])\n",
    "actor_2_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_2_facebook_likes,boundaries=[114.2, 269.0, 417.6, 542.8, 650.0, 793.2, 899.0, 1000.0, 5000.0, 137000.0])\n",
    "movie_facebook_likes_buckets = tf.feature_column.bucketized_column(movie_facebook_likes,boundaries=[182.0, 613.8, 1000.0, 14000.0, 27200.0, 349000.0])\n",
    "genres_count_buckets = tf.feature_column.bucketized_column(genres_count,boundaries=[2.0, 3.0, 4.0, 8.0])\n",
    "plotkeywords_count_buckets = tf.feature_column.bucketized_column(plotkeywords_count,boundaries=[51.0, 55.0, 59.0, 62.0, 66.0, 70.0, 74.6, 80.0, 90.0, 165.0])\n",
    "movie_title_word_length_buckets = tf.feature_column.bucketized_column(movie_title_word_length,boundaries=[2.0, 3.0, 4.0, 5.0, 13.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Defining the Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "  def parse_csv(data_file):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(data_file, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('imdb_score')\n",
    "    return features,labels\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  \n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the deep columns\n",
    "deep_columns=[genres_count_buckets,plotkeywords_count_buckets,num_critic_for_reviews_buckets,duration_buckets,director_facebook_likes_buckets,actor_3_facebook_likes_buckets,actor_2_facebook_likes_buckets,actor_1_facebook_likes_buckets,num_voted_users_buckets,facenumber_in_poster_buckets,budget_buckets,title_year_buckets,gross_buckets]\n",
    "#note user reviews and movie facebook likes and cast facebook likes are not considered for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'tmp\\\\model2_DNN_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002215B14CF98>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#defining the model\n",
    "model2=tf.estimator.DNNClassifier(\n",
    "        model_dir=\"tmp\\\\model2_DNN_\",\n",
    "        n_classes=4,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=[100,50, 25],\n",
    "        config=tf.estimator.RunConfig().replace(session_config=tf.ConfigProto(device_count={'GPU': 0}))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from tmp\\model2_DNN_\\model.ckpt-12104\n",
      "INFO:tensorflow:Saving checkpoints for 12105 into tmp\\model2_DNN_\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0369959, step = 12105\n",
      "INFO:tensorflow:global_step/sec: 243.731\n",
      "INFO:tensorflow:loss = 4.53255, step = 12205 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.185\n",
      "INFO:tensorflow:loss = 1.56565, step = 12305 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.408\n",
      "INFO:tensorflow:loss = 1.40825, step = 12405 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.321\n",
      "INFO:tensorflow:loss = 6.52362, step = 12505 (0.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 390.349\n",
      "INFO:tensorflow:loss = 0.728199, step = 12605 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.873\n",
      "INFO:tensorflow:loss = 4.11269, step = 12705 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.627\n",
      "INFO:tensorflow:loss = 0.379012, step = 12805 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.284\n",
      "INFO:tensorflow:loss = 0.929963, step = 12905 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.521\n",
      "INFO:tensorflow:loss = 0.470948, step = 13005 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.93\n",
      "INFO:tensorflow:loss = 0.236939, step = 13105 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 487.458\n",
      "INFO:tensorflow:loss = 0.733351, step = 13205 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.163\n",
      "INFO:tensorflow:loss = 0.0785763, step = 13305 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.062\n",
      "INFO:tensorflow:loss = 0.115968, step = 13405 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 444.128\n",
      "INFO:tensorflow:loss = 0.316401, step = 13505 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.815\n",
      "INFO:tensorflow:loss = 0.321902, step = 13605 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.4\n",
      "INFO:tensorflow:loss = 0.0820961, step = 13705 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.399\n",
      "INFO:tensorflow:loss = 0.22851, step = 13805 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 561.399\n",
      "INFO:tensorflow:loss = 0.0675449, step = 13905 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.261\n",
      "INFO:tensorflow:loss = 0.0144849, step = 14005 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.187\n",
      "INFO:tensorflow:loss = 0.0071457, step = 14105 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.382\n",
      "INFO:tensorflow:loss = 0.0313107, step = 14205 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 564.57\n",
      "INFO:tensorflow:loss = 0.0412856, step = 14305 (0.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.307\n",
      "INFO:tensorflow:loss = 0.126167, step = 14405 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.814\n",
      "INFO:tensorflow:loss = 0.15621, step = 14505 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.23\n",
      "INFO:tensorflow:loss = 0.00756592, step = 14605 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.093\n",
      "INFO:tensorflow:loss = 0.054756, step = 14705 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 540.157\n",
      "INFO:tensorflow:loss = 0.00425232, step = 14805 (0.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 473.596\n",
      "INFO:tensorflow:loss = 0.0207441, step = 14905 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 515.099\n",
      "INFO:tensorflow:loss = 0.108277, step = 15005 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.696\n",
      "INFO:tensorflow:loss = 0.0127514, step = 15105 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.429\n",
      "INFO:tensorflow:loss = 0.0203415, step = 15205 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.261\n",
      "INFO:tensorflow:loss = 0.00591037, step = 15305 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.645\n",
      "INFO:tensorflow:loss = 0.0165046, step = 15405 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 512.458\n",
      "INFO:tensorflow:loss = 0.0238565, step = 15505 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 502.155\n",
      "INFO:tensorflow:loss = 0.015309, step = 15605 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.261\n",
      "INFO:tensorflow:loss = 0.000413027, step = 15705 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.253\n",
      "INFO:tensorflow:loss = 0.0172703, step = 15805 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.244\n",
      "INFO:tensorflow:loss = 0.0126948, step = 15905 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.737\n",
      "INFO:tensorflow:loss = 0.00546938, step = 16005 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.332\n",
      "INFO:tensorflow:loss = 0.0086089, step = 16105 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 452.168\n",
      "INFO:tensorflow:loss = 0.0224793, step = 16205 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.187\n",
      "INFO:tensorflow:loss = 0.0337483, step = 16305 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.851\n",
      "INFO:tensorflow:loss = 0.00325324, step = 16405 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 525.941\n",
      "INFO:tensorflow:loss = 0.00749835, step = 16505 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.463\n",
      "INFO:tensorflow:loss = 0.0237166, step = 16605 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 480.428\n",
      "INFO:tensorflow:loss = 0.00281359, step = 16705 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.537\n",
      "INFO:tensorflow:loss = 0.00196622, step = 16805 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.465\n",
      "INFO:tensorflow:loss = 0.00300712, step = 16905 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.461\n",
      "INFO:tensorflow:loss = 0.019274, step = 17005 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.466\n",
      "INFO:tensorflow:loss = 0.00900017, step = 17105 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 528.723\n",
      "INFO:tensorflow:loss = 0.0199552, step = 17205 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.342\n",
      "INFO:tensorflow:loss = 0.00344546, step = 17305 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.545\n",
      "INFO:tensorflow:loss = 0.00694364, step = 17405 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.698\n",
      "INFO:tensorflow:loss = 0.00768441, step = 17505 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.464\n",
      "INFO:tensorflow:loss = 0.00262309, step = 17605 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 515.1\n",
      "INFO:tensorflow:loss = 0.0259746, step = 17705 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 534.379\n",
      "INFO:tensorflow:loss = 0.0049927, step = 17805 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 628.484\n",
      "INFO:tensorflow:loss = 0.00945902, step = 17905 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.131\n",
      "INFO:tensorflow:loss = 0.00312941, step = 18005 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.132\n",
      "INFO:tensorflow:loss = 0.00904238, step = 18105 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.489\n",
      "INFO:tensorflow:loss = 0.00388958, step = 18205 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.464\n",
      "INFO:tensorflow:loss = 0.00150189, step = 18305 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.488\n",
      "INFO:tensorflow:loss = 0.00643555, step = 18405 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.538\n",
      "INFO:tensorflow:loss = 0.00318759, step = 18505 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 653.132\n",
      "INFO:tensorflow:loss = 0.0158549, step = 18605 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 644.701\n",
      "INFO:tensorflow:loss = 0.00415834, step = 18705 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.57\n",
      "INFO:tensorflow:loss = 0.000759192, step = 18805 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.891\n",
      "INFO:tensorflow:loss = 0.00256101, step = 18905 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.488\n",
      "INFO:tensorflow:loss = 0.00127544, step = 19005 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.559\n",
      "INFO:tensorflow:loss = 0.00286554, step = 19105 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.621\n",
      "INFO:tensorflow:loss = 0.00255259, step = 19205 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.1\n",
      "INFO:tensorflow:loss = 0.00504257, step = 19305 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.89\n",
      "INFO:tensorflow:loss = 0.000235897, step = 19405 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 648.89\n",
      "INFO:tensorflow:loss = 0.000767383, step = 19505 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.677\n",
      "INFO:tensorflow:loss = 0.000582975, step = 19605 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.462\n",
      "INFO:tensorflow:loss = 0.00282463, step = 19705 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.46\n",
      "INFO:tensorflow:loss = 0.00876906, step = 19805 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.633\n",
      "INFO:tensorflow:loss = 0.00434515, step = 19905 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 580.983\n",
      "INFO:tensorflow:loss = 0.00344147, step = 20005 (0.169 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 605.629\n",
      "INFO:tensorflow:loss = 0.00372669, step = 20105 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.323\n",
      "INFO:tensorflow:loss = 0.00221622, step = 20205 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 591.295\n",
      "INFO:tensorflow:loss = 0.00106173, step = 20305 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.489\n",
      "INFO:tensorflow:loss = 0.021713, step = 20405 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 640.575\n",
      "INFO:tensorflow:loss = 0.00760369, step = 20505 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.845\n",
      "INFO:tensorflow:loss = 0.0128117, step = 20605 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.463\n",
      "INFO:tensorflow:loss = 0.00212681, step = 20705 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.677\n",
      "INFO:tensorflow:loss = 0.00203441, step = 20805 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.556\n",
      "INFO:tensorflow:loss = 0.00210248, step = 20905 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 546.06\n",
      "INFO:tensorflow:loss = 1.27354, step = 21005 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.094\n",
      "INFO:tensorflow:loss = 0.00126577, step = 21105 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.46\n",
      "INFO:tensorflow:loss = 0.0129793, step = 21205 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 517.768\n",
      "INFO:tensorflow:loss = 0.0122573, step = 21305 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.255\n",
      "INFO:tensorflow:loss = 0.00187002, step = 21405 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 515.096\n",
      "INFO:tensorflow:loss = 0.00184944, step = 21505 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.19\n",
      "INFO:tensorflow:loss = 0.00571615, step = 21605 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.06\n",
      "INFO:tensorflow:loss = 0.00815009, step = 21705 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.112\n",
      "INFO:tensorflow:loss = 0.000998564, step = 21805 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.886\n",
      "INFO:tensorflow:loss = 0.00366295, step = 21905 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.072\n",
      "INFO:tensorflow:loss = 0.00477701, step = 22005 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.189\n",
      "INFO:tensorflow:loss = 0.00245483, step = 22105 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.189\n",
      "INFO:tensorflow:loss = 0.00132179, step = 22205 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.463\n",
      "INFO:tensorflow:loss = 0.00282779, step = 22305 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 517.767\n",
      "INFO:tensorflow:loss = 0.00331839, step = 22405 (0.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.159\n",
      "INFO:tensorflow:loss = 0.00153515, step = 22505 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.253\n",
      "INFO:tensorflow:loss = 0.00162762, step = 22605 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 515.099\n",
      "INFO:tensorflow:loss = 0.00356103, step = 22705 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 509.843\n",
      "INFO:tensorflow:loss = 0.0025262, step = 22805 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 546.059\n",
      "INFO:tensorflow:loss = 0.00106019, step = 22905 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.696\n",
      "INFO:tensorflow:loss = 0.00315031, step = 23005 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.254\n",
      "INFO:tensorflow:loss = 0.00483629, step = 23105 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 504.691\n",
      "INFO:tensorflow:loss = 0.0057668, step = 23205 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.161\n",
      "INFO:tensorflow:loss = 0.00183443, step = 23305 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 492.26\n",
      "INFO:tensorflow:loss = 0.00513662, step = 23405 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 502.157\n",
      "INFO:tensorflow:loss = 0.00214978, step = 23505 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 485.093\n",
      "INFO:tensorflow:loss = 0.000735094, step = 23605 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.539\n",
      "INFO:tensorflow:loss = 0.0017374, step = 23705 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 546.059\n",
      "INFO:tensorflow:loss = 0.00158351, step = 23805 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.815\n",
      "INFO:tensorflow:loss = 0.000954901, step = 23905 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 636.49\n",
      "INFO:tensorflow:loss = 0.00301787, step = 24005 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.378\n",
      "INFO:tensorflow:loss = 0.00390895, step = 24105 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.159\n",
      "INFO:tensorflow:loss = 0.00109229, step = 24205 (0.200 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24208 into tmp\\model2_DNN_\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00151212.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x22159ea3e80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model \n",
    "model2.train(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", 40, True,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-06-16:44:50\n",
      "INFO:tensorflow:Restoring parameters from tmp\\model2_DNN_\\model.ckpt-24208\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-06-16:44:53\n",
      "INFO:tensorflow:Saving dict for global step 24208: accuracy = 0.862667, average_loss = 1.0444, global_step = 24208, loss = 10.4285\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "results2 = model2.evaluate(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", 1, False, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.862667\n",
      "average_loss: 1.0444\n",
      "global_step: 24208\n",
      "loss: 10.4285\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(results2):\n",
    "  print('%s: %s' % (key, results2[key]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
