{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the imdb rating from the movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9c858a714332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\final\\\\movie_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target is imdb score\n",
    "\n",
    "\n",
    "Removing the data where imdb score is na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[np.isfinite(df['imdb_score'])]\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in getting a count of genres and the plotkey words for the movie and how this count impacts the imdb rating\n",
    "Further, we are interested in learning the effect of title length and the first alphabet/numerical of the title and how it impacts our target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"genres_count\"]=df.genres.str.split('|', 0).apply(lambda x: len(x))\n",
    "df[\"plotkeywords_count\"]=df.plot_keywords.str.split('|', 0).apply(lambda x: len(str(x)))\n",
    "df[\"genres_array\"]=df.genres.str.split('|', 0)\n",
    "df[\"plotkeyword_1\"],df[\"plotkeyword_others\"]=df.plot_keywords.str.split('|', 1).str\n",
    "df[\"movie_title_word_length\"]=df.movie_title.str.split(' ', 0).apply(lambda x: len(x))\n",
    "df[\"movie_first_alpha_numeric\"]=pd.DataFrame(df.movie_title.str.split(' ',1).tolist(),columns = ['first','other'])[\"first\"].str[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colums ignored are\n",
    "movie_imdb_link\n",
    "plot_keywords\n",
    "movie_title\n",
    "genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(\"genres\",1)\n",
    "df=df.drop(\"movie_imdb_link\",1)\n",
    "df=df.drop(\"movie_title\",1)\n",
    "df=df.drop(\"plot_keywords\",1)\n",
    "df=df.drop(\"plotkeyword_others\",1)\n",
    "df.aspect_ratio=df.aspect_ratio.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the target rating into bins\n",
    "This is done because the ratings in the data vary by .1 but this does not necessarily have a huge impact with the audiance\n",
    "For example, a movie with 9.1 rating is as good as a movie with 9.2 rating.So, Ideally we will have to convert imdb rating into fewer bins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.imdb_score=pd.cut(df['imdb_score'],[0,5.8,6.6,7.2,10],labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test subsets. 60 train 40 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df.sample(frac=.6, random_state=1232)\n",
    "x_test = df.drop(x_train.index)\n",
    "\n",
    "x_train.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n",
    "x_test.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_CSV_COLUMNS =df.columns.values.tolist()\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in df.dtypes:\n",
    "   if(column==\"int64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0])\n",
    "   elif(column==\"float64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0.00])\n",
    "   else:\n",
    "        _CSV_COLUMN_DEFAULTS.append([''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quality Check. The value of below code must be true \n",
    "len(_CSV_COLUMNS)==len(_CSV_COLUMN_DEFAULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaing the features of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres_count = tf.feature_column.numeric_column ('genres_count')\n",
    "plotkeywords_count = tf.feature_column.numeric_column ('plotkeywords_count')\n",
    "color=tf.feature_column.categorical_column_with_vocabulary_list('color', ['Color' ' Black and White' ''])\n",
    "language=tf.feature_column.categorical_column_with_vocabulary_list('language',['English' 'Cantonese' 'Romanian' 'Spanish' 'Japanese' 'Mandarin' 'Norwegian' 'Portuguese' 'Indonesian' 'French' 'Dutch' 'Polish' 'Danish' 'Hindi' '' 'German' 'Hebrew' 'Dari' 'Hungarian' 'None' 'Arabic' 'Swedish' 'Korean' 'Zulu' 'Chinese' 'Persian' 'Italian' 'Aboriginal' 'Vietnamese' 'Telugu' 'Thai' 'Filipino' 'Mongolian' 'Aramaic' 'Kazakh' 'Maya' 'Czech' 'Dzongkha' 'Russian' 'Icelandic' 'Bosnian' 'Greek'] )\n",
    "country=tf.feature_column.categorical_column_with_vocabulary_list('country', ['USA' 'Germany' 'Hong Kong' 'UK' 'Romania' 'France' 'Mexico' 'Japan' 'China' 'Philippines' 'Norway' 'Brazil' 'Australia' 'New Zealand' 'Indonesia' 'Netherlands' 'Poland' 'Canada' 'Spain' 'Denmark' 'India' 'Argentina' 'Russia' 'Ireland' 'Czech Republic' 'Israel' 'Hungary' 'South Korea' 'South Africa' 'Sweden' 'Official site' 'Italy' 'Taiwan' 'Cameroon' 'Thailand' 'Georgia' 'Iran' 'New Line' 'Aruba' 'Afghanistan' 'Belgium' 'Iceland' 'Greece' 'Chile' 'Peru' 'West Germany' 'Colombia' 'Finland'])\n",
    "content_rating=tf.feature_column.categorical_column_with_vocabulary_list('content_rating',['PG-13' 'R' 'PG' 'G' 'Not Rated' 'Approved' 'Unrated' '' 'TV-MA' 'Passed' 'X' 'M' 'GP' 'NC-17'] )\n",
    "aspect_ratio = tf.feature_column.categorical_column_with_vocabulary_list('aspect_ratio',['2.35','1.85','2.39','1.66','','1.77','1.33','2.','2.76','1.78','1.37','2.2','1.75','2.55','2.4','2.24','1.5','16.','1.44','1.18'])\n",
    "\n",
    "director_name = tf.feature_column.categorical_column_with_hash_bucket('director_name', hash_bucket_size=5000)\n",
    "actor_2_name = tf.feature_column.categorical_column_with_hash_bucket('actor_2_name', hash_bucket_size=5000)\n",
    "actor_1_name = tf.feature_column.categorical_column_with_hash_bucket('actor_1_name', hash_bucket_size=5000)\n",
    "actor_3_name = tf.feature_column.categorical_column_with_hash_bucket('actor_3_name', hash_bucket_size=5000)\n",
    "genres_array = tf.feature_column.categorical_column_with_hash_bucket('genres_array', hash_bucket_size=len(set(functools.reduce(list.__add__, df.genres_array))))\n",
    "plotkeyword_1 = tf.feature_column.categorical_column_with_hash_bucket('plotkeyword_1', hash_bucket_size=len(set(df.plotkeyword_1)))\n",
    "movie_first_alpha_numeric = tf.feature_column.categorical_column_with_hash_bucket('movie_first_alpha_numeric', hash_bucket_size=len(set(df.movie_first_alpha_numeric)))\n",
    "num_critic_for_reviews = tf.feature_column.numeric_column ('num_critic_for_reviews')\n",
    "duration = tf.feature_column.numeric_column ('duration')\n",
    "director_facebook_likes = tf.feature_column.numeric_column ('director_facebook_likes')\n",
    "actor_3_facebook_likes = tf.feature_column.numeric_column ('actor_3_facebook_likes')\n",
    "actor_1_facebook_likes = tf.feature_column.numeric_column ('actor_1_facebook_likes')\n",
    "gross = tf.feature_column.numeric_column ('gross')\n",
    "num_voted_users = tf.feature_column.numeric_column ('num_voted_users')\n",
    "cast_total_facebook_likes = tf.feature_column.numeric_column ('cast_total_facebook_likes')\n",
    "facenumber_in_poster = tf.feature_column.numeric_column ('facenumber_in_poster')\n",
    "num_user_for_reviews = tf.feature_column.numeric_column ('num_user_for_reviews')\n",
    "budget = tf.feature_column.numeric_column ('budget')\n",
    "title_year = tf.feature_column.numeric_column ('title_year')\n",
    "actor_2_facebook_likes = tf.feature_column.numeric_column ('actor_2_facebook_likes')\n",
    "movie_facebook_likes = tf.feature_column.numeric_column ('movie_facebook_likes')\n",
    "movie_title_word_length = tf.feature_column.numeric_column ('movie_title_word_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tensor flow buckets based on the numeric columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_critic_for_reviews_buckets = tf.feature_column.bucketized_column(num_critic_for_reviews,boundaries=[35.0, 60.0, 81.0, 105.0, 130.0, 159.0, 193.0, 241.0, 323.0, 813.0])\n",
    "duration_buckets = tf.feature_column.bucketized_column(duration,boundaries=[88.0, 93.0, 97.0, 101.0, 105.0, 110.0, 116.0, 123.0, 135.0, 334.0])\n",
    "director_facebook_likes_buckets = tf.feature_column.bucketized_column(director_facebook_likes,boundaries=[4.0, 15.0, 32.0, 57.0, 98.0, 165.0, 301.0, 571.0, 23000.0])\n",
    "actor_3_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_3_facebook_likes,boundaries=[48.0, 122.0, 218.0, 311.0, 416.0, 511.0, 612.0, 745.0, 924.5, 23000.0])\n",
    "actor_1_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_1_facebook_likes,boundaries=[353.0, 600.8, 794.2, 939.0, 1000.0, 3000.0, 11000.0, 14000.0, 20000.0, 640000.0])\n",
    "gross_buckets = tf.feature_column.bucketized_column(gross,boundaries=[382601.8, 3023265.0, 8131359.8, 15802119.0, 25517500.0, 36565306.4, 51798114.8, 75601728.0, 125025163.2, 760505847.0])\n",
    "num_voted_users_buckets = tf.feature_column.bucketized_column(num_voted_users,boundaries=[4553.4, 11232.6, 20003.2, 31266.6, 46396.0, 66462.8, 93759.4, 145332.6, 247440.2, 1689764.0])\n",
    "cast_total_facebook_likes_buckets = tf.feature_column.bucketized_column(cast_total_facebook_likes,boundaries=[725.8, 1384.2, 2049.6, 2731.0, 3697.0, 5708.8, 13332.2, 18452.8, 28099.6, 656730.0])\n",
    "facenumber_in_poster_buckets = tf.feature_column.bucketized_column(facenumber_in_poster,boundaries=[1.0, 2.0, 4.0, 43.0])\n",
    "num_user_for_reviews_buckets = tf.feature_column.bucketized_column(num_user_for_reviews,boundaries=[40.0, 77.0, 111.0, 147.0, 190.0, 247.0, 320.9, 442.6, 687.0, 5060.0])\n",
    "budget_buckets = tf.feature_column.bucketized_column(budget,boundaries=[2600000.0, 7000000.0, 12000000.0, 17000000.0, 24000000.0, 30000000.0, 40000000.0, 60000000.0, 93000000.0, 12215500000.0])\n",
    "title_year_buckets = tf.feature_column.bucketized_column(title_year,boundaries=[1993.0, 1998.0, 2000.0, 2003.0, 2005.0, 2007.0, 2009.0, 2011.0, 2013.0, 2016.0])\n",
    "actor_2_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_2_facebook_likes,boundaries=[114.2, 269.0, 417.6, 542.8, 650.0, 793.2, 899.0, 1000.0, 5000.0, 137000.0])\n",
    "movie_facebook_likes_buckets = tf.feature_column.bucketized_column(movie_facebook_likes,boundaries=[182.0, 613.8, 1000.0, 14000.0, 27200.0, 349000.0])\n",
    "genres_count_buckets = tf.feature_column.bucketized_column(genres_count,boundaries=[2.0, 3.0, 4.0, 8.0])\n",
    "plotkeywords_count_buckets = tf.feature_column.bucketized_column(plotkeywords_count,boundaries=[51.0, 55.0, 59.0, 62.0, 66.0, 70.0, 74.6, 80.0, 90.0, 165.0])\n",
    "movie_title_word_length_buckets = tf.feature_column.bucketized_column(movie_title_word_length,boundaries=[2.0, 3.0, 4.0, 5.0, 13.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "  def parse_csv(data_file):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(data_file, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('imdb_score')\n",
    "    return features,labels\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  \n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the columns for deep learning and linear classifier\n",
    "base_columns = [genres_count_buckets,plotkeywords_count_buckets,plotkeyword_1,color,language,country,content_rating,director_name,actor_2_name,actor_1_name,actor_3_name,num_critic_for_reviews_buckets,duration_buckets,director_facebook_likes_buckets,actor_3_facebook_likes_buckets,actor_2_facebook_likes_buckets,actor_1_facebook_likes_buckets,num_voted_users_buckets,facenumber_in_poster_buckets,budget_buckets,title_year_buckets,gross_buckets,aspect_ratio]\n",
    "deep_columns=[genres_count_buckets,plotkeywords_count_buckets,num_critic_for_reviews_buckets,duration_buckets,director_facebook_likes_buckets,actor_3_facebook_likes_buckets,actor_2_facebook_likes_buckets,actor_1_facebook_likes_buckets,num_voted_users_buckets,cast_total_facebook_likes_buckets,facenumber_in_poster_buckets,num_user_for_reviews_buckets,budget_buckets,title_year_buckets,gross_buckets,movie_facebook_likes_buckets]\n",
    "#note user reviews and movie facebook likes and cast facebook likes are not considered for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining the model\n",
    "model3 = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir='tmp\\\\model_h2',\n",
    "    linear_feature_columns=base_columns,\n",
    "    n_classes=4,\n",
    "    linear_optimizer=tf.train.FtrlOptimizer(learning_rate=0.1,l1_regularization_strength=1.0,l2_regularization_strength=1.0),\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training the model \n",
    "model3.train(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", 40, True,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "results3 = model3.evaluate(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", 1, False, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in sorted(results3):\n",
    "  print('%s: %s' % (key, results3[key]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
