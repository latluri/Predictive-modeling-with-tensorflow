{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the imdb rating from the movie data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This script is written by Atluri Laxmi Narayana\n",
    "This script reads the file \"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\final\\\\movie_metadata.csv\" to predict the imdb score \n",
    "This code generates Hybrid model(DNN with linear combiner) in tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\final\\\\movie_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target is imdb score\n",
    "Removing the data where imdb score is na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[np.isfinite(df['imdb_score'])]\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in getting a count of genres and the plotkey words for the movie and how this count impacts the imdb rating\n",
    "Further, we are interested in learning the effect of title length and the first alphabet/numerical of the title and how it impacts our target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"genres_count\"]=df.genres.str.split('|', 0).apply(lambda x: len(x))\n",
    "df[\"plotkeywords_count\"]=df.plot_keywords.str.split('|', 0).apply(lambda x: len(str(x)))\n",
    "df[\"genres_array\"]=df.genres.str.split('|', 0)\n",
    "df[\"plotkeyword_1\"],df[\"plotkeyword_others\"]=df.plot_keywords.str.split('|', 1).str\n",
    "df[\"movie_title_word_length\"]=df.movie_title.str.split(' ', 0).apply(lambda x: len(x))\n",
    "df[\"movie_first_alpha_numeric\"]=pd.DataFrame(df.movie_title.str.split(' ',1).tolist(),columns = ['first','other'])[\"first\"].str[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colums ignored are\n",
    "movie_imdb_link\n",
    "plot_keywords\n",
    "movie_title\n",
    "genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(\"genres\",1)\n",
    "df=df.drop(\"movie_imdb_link\",1)\n",
    "df=df.drop(\"movie_title\",1)\n",
    "df=df.drop(\"plot_keywords\",1)\n",
    "df=df.drop(\"plotkeyword_others\",1)\n",
    "df.aspect_ratio=df.aspect_ratio.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the target rating into bins\n",
    "This is done because the ratings in the data vary by .1 but this does not necessarily have a huge impact with the audiance\n",
    "For example, a movie with 9.1 rating is as good as a movie with 9.2 rating.So, Ideally we will have to convert imdb rating into fewer bins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.imdb_score=pd.cut(df['imdb_score'],[0,4.0,6.0,8.0,10],labels=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test subsets. 60 train 40 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df.sample(frac=.6, random_state=1232)\n",
    "x_test = df.drop(x_train.index)\n",
    "\n",
    "x_train.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n",
    "x_test.to_csv(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", sep=',', encoding='utf-8',header=False,na_rep='',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_CSV_COLUMNS =df.columns.values.tolist()\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in df.dtypes:\n",
    "   if(column==\"int64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0])\n",
    "   elif(column==\"float64\"):\n",
    "        _CSV_COLUMN_DEFAULTS.append([0.00])\n",
    "   else:\n",
    "        _CSV_COLUMN_DEFAULTS.append([''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quality Check. The value of below code must be true \n",
    "len(_CSV_COLUMNS)==len(_CSV_COLUMN_DEFAULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaing the features of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres_count = tf.feature_column.numeric_column ('genres_count')\n",
    "plotkeywords_count = tf.feature_column.numeric_column ('plotkeywords_count')\n",
    "color=tf.feature_column.categorical_column_with_vocabulary_list('color', ['Color' ' Black and White' ''])\n",
    "language=tf.feature_column.categorical_column_with_vocabulary_list('language',['English' 'Cantonese' 'Romanian' 'Spanish' 'Japanese' 'Mandarin' 'Norwegian' 'Portuguese' 'Indonesian' 'French' 'Dutch' 'Polish' 'Danish' 'Hindi' '' 'German' 'Hebrew' 'Dari' 'Hungarian' 'None' 'Arabic' 'Swedish' 'Korean' 'Zulu' 'Chinese' 'Persian' 'Italian' 'Aboriginal' 'Vietnamese' 'Telugu' 'Thai' 'Filipino' 'Mongolian' 'Aramaic' 'Kazakh' 'Maya' 'Czech' 'Dzongkha' 'Russian' 'Icelandic' 'Bosnian' 'Greek'] )\n",
    "country=tf.feature_column.categorical_column_with_vocabulary_list('country', ['USA' 'Germany' 'Hong Kong' 'UK' 'Romania' 'France' 'Mexico' 'Japan' 'China' 'Philippines' 'Norway' 'Brazil' 'Australia' 'New Zealand' 'Indonesia' 'Netherlands' 'Poland' 'Canada' 'Spain' 'Denmark' 'India' 'Argentina' 'Russia' 'Ireland' 'Czech Republic' 'Israel' 'Hungary' 'South Korea' 'South Africa' 'Sweden' 'Official site' 'Italy' 'Taiwan' 'Cameroon' 'Thailand' 'Georgia' 'Iran' 'New Line' 'Aruba' 'Afghanistan' 'Belgium' 'Iceland' 'Greece' 'Chile' 'Peru' 'West Germany' 'Colombia' 'Finland'])\n",
    "content_rating=tf.feature_column.categorical_column_with_vocabulary_list('content_rating',['PG-13' 'R' 'PG' 'G' 'Not Rated' 'Approved' 'Unrated' '' 'TV-MA' 'Passed' 'X' 'M' 'GP' 'NC-17'] )\n",
    "aspect_ratio = tf.feature_column.categorical_column_with_vocabulary_list('aspect_ratio',['2.35','1.85','2.39','1.66','','1.77','1.33','2.','2.76','1.78','1.37','2.2','1.75','2.55','2.4','2.24','1.5','16.','1.44','1.18'])\n",
    "\n",
    "director_name = tf.feature_column.categorical_column_with_hash_bucket('director_name', hash_bucket_size=5000)\n",
    "actor_2_name = tf.feature_column.categorical_column_with_hash_bucket('actor_2_name', hash_bucket_size=5000)\n",
    "actor_1_name = tf.feature_column.categorical_column_with_hash_bucket('actor_1_name', hash_bucket_size=5000)\n",
    "actor_3_name = tf.feature_column.categorical_column_with_hash_bucket('actor_3_name', hash_bucket_size=5000)\n",
    "genres_array = tf.feature_column.categorical_column_with_hash_bucket('genres_array', hash_bucket_size=len(set(functools.reduce(list.__add__, df.genres_array))))\n",
    "plotkeyword_1 = tf.feature_column.categorical_column_with_hash_bucket('plotkeyword_1', hash_bucket_size=len(set(df.plotkeyword_1)))\n",
    "movie_first_alpha_numeric = tf.feature_column.categorical_column_with_hash_bucket('movie_first_alpha_numeric', hash_bucket_size=len(set(df.movie_first_alpha_numeric)))\n",
    "num_critic_for_reviews = tf.feature_column.numeric_column ('num_critic_for_reviews')\n",
    "duration = tf.feature_column.numeric_column ('duration')\n",
    "director_facebook_likes = tf.feature_column.numeric_column ('director_facebook_likes')\n",
    "actor_3_facebook_likes = tf.feature_column.numeric_column ('actor_3_facebook_likes')\n",
    "actor_1_facebook_likes = tf.feature_column.numeric_column ('actor_1_facebook_likes')\n",
    "gross = tf.feature_column.numeric_column ('gross')\n",
    "num_voted_users = tf.feature_column.numeric_column ('num_voted_users')\n",
    "cast_total_facebook_likes = tf.feature_column.numeric_column ('cast_total_facebook_likes')\n",
    "facenumber_in_poster = tf.feature_column.numeric_column ('facenumber_in_poster')\n",
    "num_user_for_reviews = tf.feature_column.numeric_column ('num_user_for_reviews')\n",
    "budget = tf.feature_column.numeric_column ('budget')\n",
    "title_year = tf.feature_column.numeric_column ('title_year')\n",
    "actor_2_facebook_likes = tf.feature_column.numeric_column ('actor_2_facebook_likes')\n",
    "movie_facebook_likes = tf.feature_column.numeric_column ('movie_facebook_likes')\n",
    "movie_title_word_length = tf.feature_column.numeric_column ('movie_title_word_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tensor flow buckets based on the numeric columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_critic_for_reviews_buckets = tf.feature_column.bucketized_column(num_critic_for_reviews,boundaries=[35.0, 60.0, 81.0, 105.0, 130.0, 159.0, 193.0, 241.0, 323.0, 813.0])\n",
    "duration_buckets = tf.feature_column.bucketized_column(duration,boundaries=[88.0, 93.0, 97.0, 101.0, 105.0, 110.0, 116.0, 123.0, 135.0, 334.0])\n",
    "director_facebook_likes_buckets = tf.feature_column.bucketized_column(director_facebook_likes,boundaries=[4.0, 15.0, 32.0, 57.0, 98.0, 165.0, 301.0, 571.0, 23000.0])\n",
    "actor_3_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_3_facebook_likes,boundaries=[48.0, 122.0, 218.0, 311.0, 416.0, 511.0, 612.0, 745.0, 924.5, 23000.0])\n",
    "actor_1_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_1_facebook_likes,boundaries=[353.0, 600.8, 794.2, 939.0, 1000.0, 3000.0, 11000.0, 14000.0, 20000.0, 640000.0])\n",
    "gross_buckets = tf.feature_column.bucketized_column(gross,boundaries=[382601.8, 3023265.0, 8131359.8, 15802119.0, 25517500.0, 36565306.4, 51798114.8, 75601728.0, 125025163.2, 760505847.0])\n",
    "num_voted_users_buckets = tf.feature_column.bucketized_column(num_voted_users,boundaries=[4553.4, 11232.6, 20003.2, 31266.6, 46396.0, 66462.8, 93759.4, 145332.6, 247440.2, 1689764.0])\n",
    "cast_total_facebook_likes_buckets = tf.feature_column.bucketized_column(cast_total_facebook_likes,boundaries=[725.8, 1384.2, 2049.6, 2731.0, 3697.0, 5708.8, 13332.2, 18452.8, 28099.6, 656730.0])\n",
    "facenumber_in_poster_buckets = tf.feature_column.bucketized_column(facenumber_in_poster,boundaries=[1.0, 2.0, 4.0, 43.0])\n",
    "num_user_for_reviews_buckets = tf.feature_column.bucketized_column(num_user_for_reviews,boundaries=[40.0, 77.0, 111.0, 147.0, 190.0, 247.0, 320.9, 442.6, 687.0, 5060.0])\n",
    "budget_buckets = tf.feature_column.bucketized_column(budget,boundaries=[2600000.0, 7000000.0, 12000000.0, 17000000.0, 24000000.0, 30000000.0, 40000000.0, 60000000.0, 93000000.0, 12215500000.0])\n",
    "title_year_buckets = tf.feature_column.bucketized_column(title_year,boundaries=[1993.0, 1998.0, 2000.0, 2003.0, 2005.0, 2007.0, 2009.0, 2011.0, 2013.0, 2016.0])\n",
    "actor_2_facebook_likes_buckets = tf.feature_column.bucketized_column(actor_2_facebook_likes,boundaries=[114.2, 269.0, 417.6, 542.8, 650.0, 793.2, 899.0, 1000.0, 5000.0, 137000.0])\n",
    "movie_facebook_likes_buckets = tf.feature_column.bucketized_column(movie_facebook_likes,boundaries=[182.0, 613.8, 1000.0, 14000.0, 27200.0, 349000.0])\n",
    "genres_count_buckets = tf.feature_column.bucketized_column(genres_count,boundaries=[2.0, 3.0, 4.0, 8.0])\n",
    "plotkeywords_count_buckets = tf.feature_column.bucketized_column(plotkeywords_count,boundaries=[51.0, 55.0, 59.0, 62.0, 66.0, 70.0, 74.6, 80.0, 90.0, 165.0])\n",
    "movie_title_word_length_buckets = tf.feature_column.bucketized_column(movie_title_word_length,boundaries=[2.0, 3.0, 4.0, 5.0, 13.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "  def parse_csv(data_file):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(data_file, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
    "    labels = features.pop('imdb_score')\n",
    "    return features,labels\n",
    "  # Extract lines from input files using the Dataset API.\n",
    "  \n",
    "  dataset = tf.data.TextLineDataset(data_file)\n",
    "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
    "  # epochs from blending together.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the columns for deep learning and linear classifier\n",
    "base_columns = [genres_count_buckets,plotkeywords_count_buckets,plotkeyword_1,color,language,country,content_rating,director_name,actor_2_name,actor_1_name,actor_3_name,num_critic_for_reviews_buckets,duration_buckets,director_facebook_likes_buckets,actor_3_facebook_likes_buckets,actor_2_facebook_likes_buckets,actor_1_facebook_likes_buckets,num_voted_users_buckets,facenumber_in_poster_buckets,budget_buckets,title_year_buckets,gross_buckets,aspect_ratio]\n",
    "deep_columns=[genres_count_buckets,plotkeywords_count_buckets,num_critic_for_reviews_buckets,duration_buckets,director_facebook_likes_buckets,actor_3_facebook_likes_buckets,actor_2_facebook_likes_buckets,actor_1_facebook_likes_buckets,num_voted_users_buckets,cast_total_facebook_likes_buckets,facenumber_in_poster_buckets,num_user_for_reviews_buckets,budget_buckets,title_year_buckets,gross_buckets,movie_facebook_likes_buckets]\n",
    "#note user reviews and movie facebook likes and cast facebook likes are not considered for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'tmp\\\\model_h2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B6E0917C50>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#defining the model\n",
    "model3 = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir='tmp\\\\model_h2',\n",
    "    linear_feature_columns=base_columns,\n",
    "    n_classes=4,\n",
    "    linear_optimizer=tf.train.FtrlOptimizer(learning_rate=0.1,l1_regularization_strength=1.0,l2_regularization_strength=1.0),\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into tmp\\model_h2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 15.9252, step = 1\n",
      "INFO:tensorflow:global_step/sec: 31.0534\n",
      "INFO:tensorflow:loss = 5.19765, step = 101 (3.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9287\n",
      "INFO:tensorflow:loss = 11.1067, step = 201 (1.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.031\n",
      "INFO:tensorflow:loss = 8.47626, step = 301 (0.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.108\n",
      "INFO:tensorflow:loss = 7.70221, step = 401 (0.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.405\n",
      "INFO:tensorflow:loss = 9.4637, step = 501 (0.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.81\n",
      "INFO:tensorflow:loss = 8.2385, step = 601 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.714\n",
      "INFO:tensorflow:loss = 4.5053, step = 701 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.112\n",
      "INFO:tensorflow:loss = 4.83479, step = 801 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.629\n",
      "INFO:tensorflow:loss = 5.33767, step = 901 (0.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.579\n",
      "INFO:tensorflow:loss = 4.26091, step = 1001 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.906\n",
      "INFO:tensorflow:loss = 3.51147, step = 1101 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.408\n",
      "INFO:tensorflow:loss = 6.79712, step = 1201 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.329\n",
      "INFO:tensorflow:loss = 3.296, step = 1301 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.991\n",
      "INFO:tensorflow:loss = 4.65204, step = 1401 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.491\n",
      "INFO:tensorflow:loss = 3.22113, step = 1501 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.511\n",
      "INFO:tensorflow:loss = 3.48752, step = 1601 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.735\n",
      "INFO:tensorflow:loss = 5.01501, step = 1701 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.991\n",
      "INFO:tensorflow:loss = 3.65936, step = 1801 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.992\n",
      "INFO:tensorflow:loss = 2.62871, step = 1901 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.991\n",
      "INFO:tensorflow:loss = 3.15, step = 2001 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.087\n",
      "INFO:tensorflow:loss = 3.81333, step = 2101 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.329\n",
      "INFO:tensorflow:loss = 2.96154, step = 2201 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.65\n",
      "INFO:tensorflow:loss = 2.4645, step = 2301 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.531\n",
      "INFO:tensorflow:loss = 2.56204, step = 2401 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.735\n",
      "INFO:tensorflow:loss = 1.677, step = 2501 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.424\n",
      "INFO:tensorflow:loss = 2.11903, step = 2601 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 2.31005, step = 2701 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.649\n",
      "INFO:tensorflow:loss = 2.03056, step = 2801 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.047\n",
      "INFO:tensorflow:loss = 1.48475, step = 2901 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.352\n",
      "INFO:tensorflow:loss = 2.1589, step = 3001 (0.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.89\n",
      "INFO:tensorflow:loss = 3.30408, step = 3101 (0.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.715\n",
      "INFO:tensorflow:loss = 1.464, step = 3201 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 320.285\n",
      "INFO:tensorflow:loss = 2.8652, step = 3301 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.395\n",
      "INFO:tensorflow:loss = 1.41555, step = 3401 (0.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.89\n",
      "INFO:tensorflow:loss = 1.366, step = 3501 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.462\n",
      "INFO:tensorflow:loss = 2.02139, step = 3601 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.462\n",
      "INFO:tensorflow:loss = 0.796845, step = 3701 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 327.636\n",
      "INFO:tensorflow:loss = 2.29621, step = 3801 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.332\n",
      "INFO:tensorflow:loss = 1.37227, step = 3901 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.333\n",
      "INFO:tensorflow:loss = 1.40142, step = 4001 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.798\n",
      "INFO:tensorflow:loss = 1.63396, step = 4101 (0.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.462\n",
      "INFO:tensorflow:loss = 1.14944, step = 4201 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.096\n",
      "INFO:tensorflow:loss = 1.32366, step = 4301 (0.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.892\n",
      "INFO:tensorflow:loss = 0.879458, step = 4401 (0.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 329.798\n",
      "INFO:tensorflow:loss = 1.50941, step = 4501 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.698\n",
      "INFO:tensorflow:loss = 1.65636, step = 4601 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.812\n",
      "INFO:tensorflow:loss = 1.20459, step = 4701 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.338\n",
      "INFO:tensorflow:loss = 1.08407, step = 4801 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.477\n",
      "INFO:tensorflow:loss = 0.749123, step = 4901 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.627\n",
      "INFO:tensorflow:loss = 1.25486, step = 5001 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.363\n",
      "INFO:tensorflow:loss = 1.11935, step = 5101 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.353\n",
      "INFO:tensorflow:loss = 1.28083, step = 5201 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.9\n",
      "INFO:tensorflow:loss = 0.723122, step = 5301 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 1.57201, step = 5401 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.339\n",
      "INFO:tensorflow:loss = 1.36916, step = 5501 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.776\n",
      "INFO:tensorflow:loss = 1.06785, step = 5601 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 0.995017, step = 5701 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.422\n",
      "INFO:tensorflow:loss = 1.19652, step = 5801 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.662\n",
      "INFO:tensorflow:loss = 1.07083, step = 5901 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.736\n",
      "INFO:tensorflow:loss = 1.24849, step = 6001 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.24\n",
      "INFO:tensorflow:loss = 1.27399, step = 6101 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.816\n",
      "INFO:tensorflow:loss = 1.21245, step = 6201 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.339\n",
      "INFO:tensorflow:loss = 0.731141, step = 6301 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.735\n",
      "INFO:tensorflow:loss = 0.509667, step = 6401 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.648\n",
      "INFO:tensorflow:loss = 1.16214, step = 6501 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.292\n",
      "INFO:tensorflow:loss = 0.739269, step = 6601 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.282\n",
      "INFO:tensorflow:loss = 1.05322, step = 6701 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.531\n",
      "INFO:tensorflow:loss = 0.503036, step = 6801 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.901\n",
      "INFO:tensorflow:loss = 1.34103, step = 6901 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.776\n",
      "INFO:tensorflow:loss = 1.21739, step = 7001 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.909\n",
      "INFO:tensorflow:loss = 0.601582, step = 7101 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.525\n",
      "INFO:tensorflow:loss = 1.03787, step = 7201 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.775\n",
      "INFO:tensorflow:loss = 1.06094, step = 7301 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.56\n",
      "INFO:tensorflow:loss = 0.994775, step = 7401 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.739\n",
      "INFO:tensorflow:loss = 0.662349, step = 7501 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.53\n",
      "INFO:tensorflow:loss = 1.36149, step = 7601 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.233\n",
      "INFO:tensorflow:loss = 1.46104, step = 7701 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.574\n",
      "INFO:tensorflow:loss = 0.737972, step = 7801 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.242\n",
      "INFO:tensorflow:loss = 0.713798, step = 7901 (0.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.242\n",
      "INFO:tensorflow:loss = 0.704877, step = 8001 (0.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.231\n",
      "INFO:tensorflow:loss = 0.998762, step = 8101 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.714\n",
      "INFO:tensorflow:loss = 0.833202, step = 8201 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.662\n",
      "INFO:tensorflow:loss = 0.67968, step = 8301 (0.328 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 326.565\n",
      "INFO:tensorflow:loss = 0.841143, step = 8401 (0.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 316.231\n",
      "INFO:tensorflow:loss = 0.653707, step = 8501 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.339\n",
      "INFO:tensorflow:loss = 1.07985, step = 8601 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.99\n",
      "INFO:tensorflow:loss = 0.488908, step = 8701 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.096\n",
      "INFO:tensorflow:loss = 1.02959, step = 8801 (0.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 324.444\n",
      "INFO:tensorflow:loss = 1.12701, step = 8901 (0.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 330.892\n",
      "INFO:tensorflow:loss = 0.571831, step = 9001 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.733\n",
      "INFO:tensorflow:loss = 0.740017, step = 9101 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.476\n",
      "INFO:tensorflow:loss = 0.808939, step = 9201 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 315.233\n",
      "INFO:tensorflow:loss = 0.338534, step = 9301 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.338\n",
      "INFO:tensorflow:loss = 0.706916, step = 9401 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.279\n",
      "INFO:tensorflow:loss = 1.18429, step = 9501 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 0.595123, step = 9601 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.423\n",
      "INFO:tensorflow:loss = 0.574961, step = 9701 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.306\n",
      "INFO:tensorflow:loss = 0.872611, step = 9801 (0.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.735\n",
      "INFO:tensorflow:loss = 0.601703, step = 9901 (0.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.531\n",
      "INFO:tensorflow:loss = 0.708022, step = 10001 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.378\n",
      "INFO:tensorflow:loss = 0.371935, step = 10101 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.99\n",
      "INFO:tensorflow:loss = 0.605935, step = 10201 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.475\n",
      "INFO:tensorflow:loss = 0.494778, step = 10301 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.473\n",
      "INFO:tensorflow:loss = 0.592376, step = 10401 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.593\n",
      "INFO:tensorflow:loss = 0.568991, step = 10501 (0.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.899\n",
      "INFO:tensorflow:loss = 1.33317, step = 10601 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.341\n",
      "INFO:tensorflow:loss = 0.950387, step = 10701 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.524\n",
      "INFO:tensorflow:loss = 0.810176, step = 10801 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.474\n",
      "INFO:tensorflow:loss = 0.841134, step = 10901 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.473\n",
      "INFO:tensorflow:loss = 0.870791, step = 11001 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.189\n",
      "INFO:tensorflow:loss = 0.630589, step = 11101 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.378\n",
      "INFO:tensorflow:loss = 0.73915, step = 11201 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.474\n",
      "INFO:tensorflow:loss = 1.29532, step = 11301 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.661\n",
      "INFO:tensorflow:loss = 0.811712, step = 11401 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.306\n",
      "INFO:tensorflow:loss = 1.0289, step = 11501 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.278\n",
      "INFO:tensorflow:loss = 0.400937, step = 11601 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 303.734\n",
      "INFO:tensorflow:loss = 0.484268, step = 11701 (0.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.378\n",
      "INFO:tensorflow:loss = 0.622092, step = 11801 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.34\n",
      "INFO:tensorflow:loss = 0.591521, step = 11901 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 327.636\n",
      "INFO:tensorflow:loss = 0.681411, step = 12001 (0.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.461\n",
      "INFO:tensorflow:loss = 0.595245, step = 12101 (0.298 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12104 into tmp\\model_h2\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.575029.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier at 0x1b6e2eed6d8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model \n",
    "model3.train(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_train.csv\", 40, True,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-06-16:53:51\n",
      "INFO:tensorflow:Restoring parameters from tmp\\model_h2\\model.ckpt-12104\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-06-16:54:00\n",
      "INFO:tensorflow:Saving dict for global step 12104: accuracy = 0.701041, average_loss = 0.778041, global_step = 12104, loss = 7.76885\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "results3 = model3.evaluate(input_fn=lambda: input_fn(\"C:\\\\Users\\\\Pavan\\\\desktop\\\\rang\\\\tf2\\\\x_test.csv\", 1, False, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.701041\n",
      "average_loss: 0.778041\n",
      "global_step: 12104\n",
      "loss: 7.76885\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(results3):\n",
    "  print('%s: %s' % (key, results3[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
